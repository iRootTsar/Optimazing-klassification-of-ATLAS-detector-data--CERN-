{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is final code where we run the best modified CNN model with different data agumentation techniques as well as combined data augmentation to see if we introduce bias into classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#torch \n",
    "import torch\n",
    "import torchvision as torchv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metode for å hente data frå mappe\n",
    "module_path = str(Path.cwd().parents[0].parents[0] / \"methods\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from dataloader import *\n",
    "# from plotCreator import *\n",
    "\n",
    "data_path0 = str(Path.cwd().parents[0].parents[0] / \"data\" / \"BH_n4_M10_res50_15000_events.h5\")\n",
    "data_path1 = str(Path.cwd().parents[0].parents[0] / \"data\" / \"PP13-Sphaleron-THR9-FRZ15-NB0-NSUBPALL_res50_15000_events.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hente data og sette til array\n",
    "bhArray = dataToArray(data_path0)\n",
    "sphArray = dataToArray(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kombinerer dataene for å kunne kjøre gjennom modellen på et samlet datasett\n",
    "dataArray = np.concatenate((bhArray,sphArray),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label array where BH is 0 and Sphalerons are 1\n",
    "labelsArray = np.concatenate((np.zeros(np.shape(bhArray)[0]),np.ones(np.shape(sphArray)[0])),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if device is running on GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data 75% into train and 25% into test\n",
    "trainData, testData, trainLabels, testLabels = train_test_split(dataArray, labelsArray, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation process where we flip horizontally, and rotate images by 180 degrees randomly thus increasing training set.\n",
    "As well as we siplace images top down on y axes\n",
    "\n",
    "Commented out part where we crop images in width which can potentially increase models precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shift_y(img, roll_axis):\n",
    "    shift = torch.randint(low=0, high=list(img.shape)[roll_axis], size=(1, 1)).item()\n",
    "    img = torch.roll(img, shift, roll_axis)\n",
    "    return img\n",
    "\n",
    "def augment_images(images):\n",
    "    images = np.moveaxis(images, -1, 1)\n",
    "    augmented_images = []\n",
    "    for img in images:\n",
    "        img = torch.from_numpy(img)\n",
    "        \n",
    "        img_flipped_horizontal = torch.flip(img, dims=[2])\n",
    "        img_flipped_vertical = random_shift_y(img, roll_axis=1)\n",
    "        img_rotated_180 = torch.rot90(img, 2, dims=[1, 2])\n",
    "\n",
    "        augmented_images.extend(\n",
    "            [img, img_flipped_horizontal, img_flipped_vertical, img_rotated_180]\n",
    "        )\n",
    "    return torch.stack(augmented_images)  # Use torch.stack instead of np.array\n",
    "\n",
    "# Apply data augmentation only on the training set\n",
    "augmented_trainData = augment_images(trainData)\n",
    "\n",
    "# Repeat the labels for the training set to match the augmented dataset\n",
    "trainLabels_repeated = np.repeat(trainLabels, 4)\n",
    "\n",
    "# Convert the augmented training data and repeated labels to PyTorch tensors\n",
    "trainLabels_repeated = torch.from_numpy(trainLabels_repeated)\n",
    "\n",
    "# Create a TensorDataset from the augmented training data and repeated labels\n",
    "train_augmented = torch.utils.data.TensorDataset(augmented_trainData, trainLabels_repeated)\n",
    "\n",
    "# Create a DataLoader for the augmented training dataset with shuffle enabled and batch size of 50\n",
    "trainLoader_augmented = DataLoader(train_augmented, shuffle=True, batch_size=200)\n",
    "\n",
    "# Apply the same preprocessing step to the test set as the training set\n",
    "testData_processed = np.moveaxis(testData, -1, 1)\n",
    "\n",
    "# Create a TensorDataset for the test data\n",
    "test = torch.utils.data.TensorDataset(torch.from_numpy(testData_processed), torch.from_numpy(testLabels))\n",
    "\n",
    "# Create a DataLoader for the test data with shuffle and batch size\n",
    "testLoader = DataLoader(test, shuffle=True, batch_size=200)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows us to look on orignial data image as well as its augmented forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(original_images, augmented_images, labels, title, augmentation_info=None):\n",
    "    n_images = len(original_images)\n",
    "    n_columns = 4\n",
    "    n_rows = n_images\n",
    "\n",
    "    augmented_types = ['Original', 'Flip Vertical', 'Displace up/down', 'Rotate 180']\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_columns, figsize=(3 * n_columns, 3 * n_rows))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j in range(n_columns):\n",
    "            if j == 0:\n",
    "                img = original_images[i]\n",
    "            else:\n",
    "                img = augmented_images[4 * i + j]\n",
    "\n",
    "            if isinstance(img, np.ndarray):\n",
    "                img_rgb = np.moveaxis(img, 0, -1) if img.shape[0] == 3 else img\n",
    "            elif isinstance(img, torch.Tensor):\n",
    "                img_rgb = np.moveaxis(img.numpy(), 0, -1)\n",
    "\n",
    "            ax = axs[i, j]\n",
    "            ax.imshow(img_rgb)\n",
    "            ax.set_title(f\"{augmented_types[j]} - Label: {'BH' if labels[i] == 1 else 'SPH'}\") #Change labels such that if 1 Bh if 1 then Sph\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "n_samples = 4\n",
    "original_images = trainData[:n_samples]\n",
    "original_labels = trainLabels[:n_samples]\n",
    "augmented_images = augmented_trainData[:4 * n_samples]\n",
    "\n",
    "visualize_comparison(original_images, augmented_images, original_labels, title=\"Original and Augmented Images\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a call on nnmodel.py in methods fodler where all our models resign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnmodel\n",
    "\n",
    "model = nnmodel.ConvModelFPLMod(0.3).to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Print the model summary, make sure to provide appropriate input size (3, 50, 50) for the 3-channel 50x50 images\n",
    "summary(model, (3, 50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(augmentation_types, all_metrics):\n",
    "    \n",
    "    metric_names = list(all_metrics[0].keys())\n",
    "    num_metrics = len(metric_names)\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    plot_index = 1\n",
    "    for metric_name in metric_names:\n",
    "        plt.subplot(num_metrics//2, 2, plot_index)\n",
    "        for i, augment_type in enumerate(augmentation_types):\n",
    "            plt.plot(all_metrics[i][metric_name], label=f'{augment_type} {metric_name}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.title(f'{metric_name} with Different Augmentations')\n",
    "        plot_index += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def predict_best_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels\n",
    "       \n",
    "def plot_average_confusion_matrix(augment_type, test_loader, device, best_models):\n",
    "    all_conf_matrices = []\n",
    "    \n",
    "    for best_model_state in best_models:\n",
    "        model = nnmodel.ConvModelFPLMod(0.3).to(device)\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.to(device)\n",
    "        all_preds, all_labels = predict_best_model(model, test_loader, device)\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=range(2))\n",
    "        all_conf_matrices.append(cm)\n",
    "    \n",
    "    avg_cm = np.mean(all_conf_matrices, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(avg_cm, cmap=plt.cm.Blues)\n",
    "    plt.title(f'Average Confusion Matrix for {augment_type} Augmentation')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks([0, 1], ['Black Holes', 'Sphalerons'])\n",
    "    plt.yticks([0, 1], ['Black Holes', 'Sphalerons'])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, int(avg_cm[i, j]), ha='center', va='center', color='black', fontsize=16)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_augmentations(augmentation_types, num_runs=7):\n",
    "    \n",
    "    augmentation_types = [\"Combined\"] + augmentation_types\n",
    "    metrx = {aug_type: [] for aug_type in augmentation_types}\n",
    "    all_metrics = []\n",
    "    all_best_models = []\n",
    "    all_best_all_preds = []\n",
    "    all_best_all_labels = []\n",
    "    best_all_metrics_for_runs = []\n",
    "\n",
    "\n",
    "    for augment_type in augmentation_types:\n",
    "        n_epochs = 50\n",
    "        avg_metrics = {'train_losses': [0.0] * n_epochs, 'train_accs': [0.0] * n_epochs, 'test_losses': [0.0] * n_epochs, 'test_accs': [0.0] * n_epochs, 'black_holes_accs': [0.0] * n_epochs, 'sphalerons_accs': [0.0] * n_epochs, 'precisions': [0.0] * n_epochs, 'recalls': [0.0] * n_epochs}\n",
    "\n",
    "        best_models_for_runs = []\n",
    "        best_metrics_for_runs = []  \n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\"\\nTraining with {augment_type} augmentation - Run {run + 1}\")\n",
    "\n",
    "            if augment_type == \"Combined\":\n",
    "                trainLoader_filtered = trainLoader_augmented\n",
    "            else:\n",
    "                # Filter the augmented training data based on the augmentation type\n",
    "                filter_indices = list(range(0, len(augmented_trainData), 4))  # Original images are always included\n",
    "                if augment_type != \"None\":\n",
    "                    aug_type_index = augmentation_types.index(augment_type) - 1\n",
    "                    filter_indices += list(range(aug_type_index + 1, len(augmented_trainData), 4))\n",
    "                filtered_trainData = augmented_trainData[filter_indices] \n",
    "                filtered_trainLabels = trainLabels_repeated[filter_indices]\n",
    "\n",
    "                # Create a TensorDataset and DataLoader for the filtered training data\n",
    "                train_filtered = torch.utils.data.TensorDataset(filtered_trainData, filtered_trainLabels)\n",
    "                trainLoader_filtered = DataLoader(train_filtered, shuffle=True, batch_size=500)\n",
    "\n",
    "            # Train the model with the filtered training data\n",
    "            model = nnmodel.ConvModelFPLMod(0.3).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "            criterion = nn.CrossEntropyLoss().to(device)\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True, factor=0.5)\n",
    "\n",
    "            metrics, best_model, best_all_preds, best_all_labels = train.train(model, trainLoader_filtered, testLoader, optimizer, criterion, n_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "            metrx[augment_type].append(metrics)\n",
    "            \n",
    "            best_epoch = np.argmin([m['test_losses'] for m in metrics])\n",
    "            \n",
    "            best_metrics_for_runs.append(metrics[best_epoch])\n",
    "            \n",
    "            # Accumulate the metrics for averaging for ploting\n",
    "            for key in avg_metrics.keys():\n",
    "                for epoch in range(n_epochs):\n",
    "                    avg_metrics[key][epoch] += metrics[epoch][key]\n",
    "\n",
    "            best_models_for_runs.append(best_model)\n",
    "\n",
    "        best_all_metrics_for_runs.append(best_metrics_for_runs)\n",
    "        \n",
    "        # Average the metrics\n",
    "        for key in avg_metrics.keys():\n",
    "            avg_metrics[key] = [x / num_runs for x in avg_metrics[key]]\n",
    "\n",
    "        all_metrics.append(avg_metrics)\n",
    "        all_best_models.append(best_models_for_runs)\n",
    "        all_best_all_preds.append(best_all_preds)\n",
    "        all_best_all_labels.append(best_all_labels)\n",
    "    \n",
    "    plot_metrics(augmentation_types, all_metrics)\n",
    "        \n",
    "    # Plot confusion matrices for each augmentation type\n",
    "    for i, augment_type in enumerate(augmentation_types):\n",
    "        best_models = all_best_models[i]\n",
    "        plot_average_confusion_matrix(augment_type, testLoader, device, best_models)  # Pass the best_all_preds and best_all_labels to the function\n",
    "\n",
    "\n",
    "    return metrx, all_metrics, best_models_for_runs, best_all_metrics_for_runs, all_best_all_preds, all_best_all_labels\n",
    "\n",
    "\n",
    "# Train and compare the model with different data augmentations\n",
    "augmentation_types = [\"None\", \"Flip Horizontal\", \"Random Shift Y\", \"Rotate 180\"]\n",
    "metr, all_metrics1, best_models_for_runs1, best_all_metrics_for_runs1, all_best_all_preds1, all_best_all_labels1 = train_with_augmentations(augmentation_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_table(metrx):\n",
    "    results = []\n",
    "    \n",
    "    for aug_type, runs_metrics in metrx.items():\n",
    "        n_runs = len(runs_metrics)\n",
    "        n_epochs = len(runs_metrics[0])\n",
    "        avg_metrics = {}\n",
    "        std_metrics = {}\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_metrics = {}\n",
    "            for run_metrics in runs_metrics:\n",
    "                for key, value in run_metrics[epoch].items():\n",
    "                    if key not in [\"all_preds\", \"all_labels\", \"epoch\"]:\n",
    "                        if key not in epoch_metrics:\n",
    "                            epoch_metrics[key] = []\n",
    "                        epoch_metrics[key].append(value)\n",
    "\n",
    "            for key, values in epoch_metrics.items():\n",
    "                if key not in avg_metrics:\n",
    "                    avg_metrics[key] = []\n",
    "                    std_metrics[key] = []\n",
    "                avg_metrics[key].append(np.mean(values))\n",
    "                std_metrics[key].append(np.std(values))\n",
    "\n",
    "        results.append({\n",
    "            'aug_type': aug_type,\n",
    "            'n_runs': n_runs,\n",
    "            'avg_metrics': avg_metrics,\n",
    "            'std_metrics': std_metrics\n",
    "        })\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for res in results:\n",
    "        data = {'Epoch': list(range(1, n_epochs + 1))}\n",
    "        for key in res['avg_metrics'].keys():\n",
    "            if key == \"Epoch\":\n",
    "                data[f\"{key}\"] = res['avg_metrics'][key]\n",
    "            else:\n",
    "                data[f\"{key}\"] = [f\"{avg:.2f} ± {std:.2f}\" for avg, std in zip(res['avg_metrics'][key], res['std_metrics'][key])]\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        \n",
    "        # Calculate total overall average and standard deviation for each metric and append to DataFrame\n",
    "        total_avg_metrics = {key: f\"{np.mean(values):.2f} ± {np.std(values):.2f}\" for key, values in res['avg_metrics'].items() if key != 'Epoch'}\n",
    "        total_avg_metrics['Epoch'] = 'Overall'\n",
    "        df_to_append = pd.DataFrame(total_avg_metrics, index=[0])\n",
    "        df = pd.concat([df, df_to_append], ignore_index=True)\n",
    "        \n",
    "        df_list.append((res['aug_type'], df))\n",
    "\n",
    "    for aug_type, df in df_list:\n",
    "        print(f\"Augmentation type {aug_type} - Nr runs: {n_runs}\")\n",
    "        display(df)\n",
    "\n",
    "create_results_table(metr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT255",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

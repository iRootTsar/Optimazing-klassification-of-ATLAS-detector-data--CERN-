{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#torch \n",
    "import torch\n",
    "import torchvision as torchv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "from torch.utils import data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metode for å hente data frå mappe\n",
    "module_path = str(Path.cwd().parents[0].parents[0] / \"methods\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from dataloader import *\n",
    "# from plotCreator import *\n",
    "\n",
    "data_path0 = str(Path.cwd().parents[0].parents[0] / \"data\" / \"BH_n4_M10_res50_15000_events.h5\")\n",
    "data_path1 = str(Path.cwd().parents[0].parents[0] / \"data\" / \"PP13-Sphaleron-THR9-FRZ15-NB0-NSUBPALL_res50_15000_events.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hente data og sette til array\n",
    "bhArray = dataToArray(data_path0)\n",
    "sphArray = dataToArray(data_path1)\n",
    "\n",
    "# print(bhArray.shape)\n",
    "# print(sphArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kombinerer dataene for å kunne kjøre gjennom modellen på et samlet datasett\n",
    "dataArray = np.concatenate((bhArray,sphArray),axis=0)\n",
    "\n",
    "# np.shape(dataArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeler tabelle med 1 og 0 (0 = svart hull, 1 = spahleron)\n",
    "labelsArray = np.concatenate((np.zeros(np.shape(bhArray)[0]),np.ones(np.shape(sphArray)[0])),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "#Sjekke om device GPU er på\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data 75% i train og 25% i test\n",
    "trainData, testData, trainLabels, testLabels = train_test_split(dataArray, labelsArray, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transofrm from numpy array to torch\n",
    "trainData = torch.from_numpy(trainData)\n",
    "testData = torch.from_numpy(testData)\n",
    "trainLabels = torch.from_numpy(trainLabels)\n",
    "testLabels = torch.from_numpy(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformere numpy til tensor\n",
    "train = torch.utils.data.TensorDataset(trainData, trainLabels)\n",
    "test = torch.utils.data.TensorDataset(testData, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "#Transform data data to dataloader\n",
    "trainLoader = DataLoader(train, shuffle=True, batch_size=50)\n",
    "testLoader = DataLoader(test, shuffle=True, batch_size=50)\n",
    "\n",
    "print(len(trainLoader))\n",
    "print(len(testLoader))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noen ting kan prøve for å forbedre modellen?\n",
    "- Data augmentering\n",
    "- Adjustnr layers, filters, kernel size in CNN\n",
    "- Learning rate scheduling\n",
    "- Wight initialization\n",
    "- Use more advanced optimizer\n",
    "- Regularization\n",
    "- Early stopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Symmetric net følgjer ikke noen konkret struktur til ResNet eller VGG net men har noen \n",
    "#karakteristikk til VGG med bruk av små konsolusjonslager med flere lag. \n",
    "#Og bruk av average poolign i staden for max pooling\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNetSymmetric(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(ResNetSymmetric, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.AvgPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.layer1 = BasicResBlock(64, 128, stride=2)\n",
    "        self.layer2 = BasicResBlock(128, 256, stride=2)\n",
    "        self.layer3 = BasicResBlock(256, 512, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x_flipped_horizontal = torch.flip(x, [3])  # flip horizontally\n",
    "        x_flipped_vertical = torch.flip(x, [2])  # flip vertically\n",
    "        x_rotated_180 = torch.rot90(x, 2, [2, 3])  # rotate 180\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        x_flipped_horizontal = self.pool1(x_flipped_horizontal)\n",
    "        x_flipped_vertical = self.pool1(x_flipped_vertical)\n",
    "        x_rotated_180 = self.pool1(x_rotated_180)\n",
    "        \n",
    "        x = (x + x_flipped_horizontal + x_flipped_vertical + x_rotated_180) / 4\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.reshape(-1, 512 * 2 * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 50, 50]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 50, 50]             128\n",
      "         AvgPool2d-3           [-1, 64, 25, 25]               0\n",
      "         AvgPool2d-4           [-1, 64, 25, 25]               0\n",
      "         AvgPool2d-5           [-1, 64, 25, 25]               0\n",
      "         AvgPool2d-6           [-1, 64, 25, 25]               0\n",
      "           Dropout-7           [-1, 64, 25, 25]               0\n",
      "            Conv2d-8          [-1, 128, 13, 13]          73,728\n",
      "       BatchNorm2d-9          [-1, 128, 13, 13]             256\n",
      "           Conv2d-10          [-1, 128, 13, 13]         147,456\n",
      "      BatchNorm2d-11          [-1, 128, 13, 13]             256\n",
      "           Conv2d-12          [-1, 128, 13, 13]         147,456\n",
      "      BatchNorm2d-13          [-1, 128, 13, 13]             256\n",
      "           Conv2d-14          [-1, 128, 13, 13]           8,192\n",
      "      BatchNorm2d-15          [-1, 128, 13, 13]             256\n",
      "    BasicResBlock-16          [-1, 128, 13, 13]               0\n",
      "          Dropout-17          [-1, 128, 13, 13]               0\n",
      "           Conv2d-18            [-1, 256, 7, 7]         294,912\n",
      "      BatchNorm2d-19            [-1, 256, 7, 7]             512\n",
      "           Conv2d-20            [-1, 256, 7, 7]         589,824\n",
      "      BatchNorm2d-21            [-1, 256, 7, 7]             512\n",
      "           Conv2d-22            [-1, 256, 7, 7]         589,824\n",
      "      BatchNorm2d-23            [-1, 256, 7, 7]             512\n",
      "           Conv2d-24            [-1, 256, 7, 7]          32,768\n",
      "      BatchNorm2d-25            [-1, 256, 7, 7]             512\n",
      "    BasicResBlock-26            [-1, 256, 7, 7]               0\n",
      "          Dropout-27            [-1, 256, 7, 7]               0\n",
      "           Conv2d-28            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-30            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-31            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-32            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-33            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-34            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-35            [-1, 512, 4, 4]           1,024\n",
      "    BasicResBlock-36            [-1, 512, 4, 4]               0\n",
      "          Dropout-37            [-1, 512, 4, 4]               0\n",
      "           Linear-38                  [-1, 256]         524,544\n",
      "          Dropout-39                  [-1, 256]               0\n",
      "           Linear-40                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 8,447,554\n",
      "Trainable params: 8,447,554\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 7.20\n",
      "Params size (MB): 32.22\n",
      "Estimated Total Size (MB): 39.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = ResNetSymmetric(dropout=0.25).to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Print the model summary, make sure to provide appropriate input size (3, 50, 50) for the 3-channel 50x50 images\n",
    "summary(model, input_size=(3, 50, 50))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koden under utfører treningen og plotter alt i noen utvalgte grafer, må ryddes i for å få bedre oversikt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, criterion, n_epochs):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize lists to store the metrics\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    black_holes_accs = []\n",
    "    sphalerons_accs = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.permute(0, 3, 1, 2)).float()\n",
    "            print(outputs.shape)\n",
    "            print(labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100 * correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        black_holes_correct = 0\n",
    "        sphalerons_correct = 0\n",
    "        black_holes_total = 0\n",
    "        sphalerons_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device).float(), labels.to(device).long()\n",
    "                outputs = model(inputs.permute(0, 3, 1, 2)).float()\n",
    "                print(outputs.shape)\n",
    "                print(labels.shape)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                # Separate accuracies\n",
    "                black_holes_correct += (predicted * labels).sum().item()\n",
    "                sphalerons_correct += ((1 - predicted) * (1 - labels)).sum().item()\n",
    "                black_holes_total += labels.sum().item()\n",
    "                sphalerons_total += (1 - labels).sum().item()\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "        black_holes_acc = 100 * black_holes_correct / black_holes_total\n",
    "        sphalerons_acc = 100 * sphalerons_correct / sphalerons_total\n",
    "        test_loss /= len(test_loader)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "        # Append the metrics to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        black_holes_accs.append(black_holes_acc)\n",
    "        sphalerons_accs.append(sphalerons_acc)\n",
    "\n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Test Loss')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(test_accs, label='Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Train and Test Accuracy')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(black_holes_accs, label='Black Holes Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Black Holes Accuracy')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(sphalerons_accs, label='Sphalerons Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Sphalerons Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks([0, 1], ['Black Holes', 'Sphalerons'])\n",
    "    plt.yticks([0, 1], ['Black Holes', 'Sphalerons'])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, cm[i, j], ha='center', va='center', color='red', fontsize=16)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 2])\n",
      "torch.Size([50])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (200) to match target batch_size (50).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     11\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m train(model, trainLoader, testLoader, optimizer, criterion, n_epochs)\n",
      "Cell \u001b[1;32mIn[39], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, optimizer, criterion, n_epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(labels\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 30\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     31\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (200) to match target batch_size (50)."
     ]
    }
   ],
   "source": [
    "# Set the learning rate and number of epochs\n",
    "n_epochs = 5\n",
    "\n",
    "# Create the model\n",
    "model = ResNetSymmetric(0.5).to(device)\n",
    "\n",
    "# Set up the optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model, trainLoader, testLoader, optimizer, criterion, n_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below makes it possible to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the trained model\n",
    "# model_path = 'model.pth'\n",
    "\n",
    "# Train and save the model\n",
    "# train(model, train_loader, test_loader, optimizer, criterion, n_epochs)\n",
    "# torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path to load the saved model\n",
    "# model_path = 'model.pth'\n",
    "\n",
    "# # Create an instance of the model and load the saved state dictionary\n",
    "# model = SymmetricNet(dropout=0.25)\n",
    "# model.load_state_dict(torch.load(model_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT255",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
